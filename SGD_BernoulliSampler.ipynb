{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T13:25:49.059682Z",
     "start_time": "2024-04-24T13:25:44.317087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x116898610>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable, grad\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T14:47:58.576551Z",
     "start_time": "2024-04-24T14:47:58.556042Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "class BernoulliSampler(Sampler):\n",
    "    def __init__(self, data_source, p=0.5):\n",
    "        \"\"\" 伯努利采样器初始化\n",
    "        参数:\n",
    "        data_source (Dataset): 数据集，用于确定采样总数\n",
    "        p (float): 每个样本被选中的概率\n",
    "        \"\"\"\n",
    "        self.data_source = data_source\n",
    "        self.p = p\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = len(self.data_source)\n",
    "        # 生成随机决定，1 表示选择该索引，0 表示不选择\n",
    "        selection = torch.bernoulli(torch.full((n,), self.p))\n",
    "        # 选择被选中的索引\n",
    "        return iter([i for i, flag in enumerate(selection) if flag == 1])\n",
    "\n",
    "    def __len__(self):\n",
    "        # 期望长度是总长度乘以选择概率 p\n",
    "        return int(len(self.data_source) * self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T14:48:12.008318Z",
     "start_time": "2024-04-24T14:48:11.982059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample counts: [1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 0. 0. 0.]\n",
      "Sample indices with duplicates: []\n"
     ]
    }
   ],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.arange(1, 101)  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# 创建数据集\n",
    "dataset = SimpleDataset()\n",
    "# 设置抽样参数\n",
    "p = 0.8\n",
    "# 创建 BernoulliSampler 实例\n",
    "sampler = BernoulliSampler(dataset, p=p)\n",
    "# 创建数据加载器\n",
    "loader = DataLoader(dataset, sampler=sampler)\n",
    "\n",
    "# 初始化样本计数数组\n",
    "sample_counts = np.zeros(len(dataset)) \n",
    "\n",
    "# 开始迭代加载器中的数据\n",
    "for batch in loader:\n",
    "    # 获取当前批次中的索引\n",
    "    idx = batch[0].item()\n",
    "    # 计数数组中对应索引位置加1\n",
    "    sample_counts[idx-1] += 1\n",
    "\n",
    "# 输出样本计数和重复索引\n",
    "print(\"Sample counts:\", sample_counts)\n",
    "print(\"Sample indices with duplicates:\", np.where(sample_counts > 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import numpy as np\n",
    "\n",
    "class BatchBernoulliSampler(Sampler):\n",
    "\n",
    "    \"\"\"\n",
    "        实现有放回的伯努利抽样\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, batch_size = 10, p=0.7):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.p = p  \n",
    "\n",
    "    def __iter__(self):\n",
    "        n = len(self.data_source)\n",
    "        \n",
    "        while True:\n",
    "            # 首先随机选择batch_size个样本（有放回）\n",
    "            batch_indices = torch.randint(0, n, (self.batch_size,)).tolist()\n",
    "            # 对这些样本进行伯努利抽样\n",
    "            final_batch = [idx for idx in batch_indices if torch.rand(1).item() < self.p]\n",
    "            if len(final_batch) > 0:\n",
    "                yield final_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.data_source) * self.p / self.batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:39:03.679779Z",
     "start_time": "2024-04-24T13:39:03.663231Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 16,  38,  90,  23,  42,   1,  20,  28,  61,  36,  85,  29,  58,  66,\n",
      "         96,  58, 100,  91,  31,  52,  14,  61,  12,  38,   5,  65,  74,  55,\n",
      "         59,  47,  33,   9,  68,  29,  52,  43,  76,  54,  99,  36,  73,  39,\n",
      "          9,  55,  70,  85,  11,  20,  74,  49,  81,  80,  13,  87,  69,  82,\n",
      "         57,  43,  59,  63,  60,  80,  54,  37,   1,  13,  19,  15,  85,  63,\n",
      "         89,  17,  79,   8,  85,  44,  95,  24,  36]) 79\n",
      "tensor([ 68,  71,  52,  23,  39,  14,  52,  68,  12,   4,  12,   7,  23,  55,\n",
      "         16,  87,  62,  72,  34,  50,  33,  25,  99,   1,  46,  91,  61,  26,\n",
      "         53,   7,  56,  58,  29,  11, 100,  13,  12,  35,  58,  18,  65,  17,\n",
      "         73,  76,  90,  28,  95,  37,  61,  92,  91,  78,  38,  66,  35,  46,\n",
      "         78,  55,   3,  99,   3,  70,  88,  88,  73,  67,  81,  78,  54,  56,\n",
      "         34,  21,  26,  62,  41,  99,  39,  14,  31,  68]) 80\n",
      "tensor([90, 54, 36, 87, 15, 84, 68,  1, 31, 95, 17, 65, 90, 48, 20, 15, 30, 79,\n",
      "        35, 80, 37, 22, 10, 72, 67, 53, 84, 26, 22, 76, 29, 59, 75, 34, 67, 94,\n",
      "         6, 88, 94, 15,  1, 92, 88, 47, 23, 68, 26, 60, 16, 31, 55, 24, 25, 96,\n",
      "        70, 19, 74, 61, 47, 17, 26, 89,  7, 79, 85, 67, 42, 89, 25, 65, 91, 28,\n",
      "         1, 93, 20, 79, 91, 24, 45, 53, 95]) 81\n",
      "Sample counts: [6. 0. 2. 1. 1. 1. 3. 1. 2. 1. 2. 4. 3. 3. 4. 3. 4. 1. 2. 4. 1. 2. 4. 3.\n",
      " 3. 5. 0. 3. 4. 1. 4. 0. 2. 3. 3. 4. 3. 3. 3. 0. 1. 2. 2. 1. 1. 2. 3. 1.\n",
      " 1. 1. 0. 4. 3. 4. 5. 2. 1. 4. 3. 2. 5. 2. 2. 0. 4. 2. 4. 6. 1. 3. 1. 2.\n",
      " 3. 3. 1. 3. 0. 3. 4. 3. 2. 1. 0. 2. 5. 0. 3. 4. 3. 4. 5. 2. 1. 2. 4. 2.\n",
      " 0. 0. 4. 2.]\n",
      "Sample indices with duplicates: [ 0  2  6  8 10 11 12 13 14 15 16 18 19 21 22 23 24 25 27 28 30 32 33 34\n",
      " 35 36 37 38 41 42 45 46 51 52 53 54 55 57 58 59 60 61 62 64 65 66 67 69\n",
      " 71 72 73 75 77 78 79 80 83 84 86 87 88 89 90 91 93 94 95 98 99]\n"
     ]
    }
   ],
   "source": [
    "dataset = SimpleDataset()\n",
    "batch_size = len(dataset)\n",
    "p = 0.8  \n",
    "sampler = BatchBernoulliSampler(dataset, batch_size=batch_size, p=p)\n",
    "\n",
    "loader = DataLoader(dataset, batch_sampler=sampler)\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "sample_counts = np.zeros(len(dataset)) \n",
    "cnt = 0\n",
    "for batch in loader:\n",
    "    print(batch, len(batch))  \n",
    "    for idx in batch:\n",
    "        sample_counts[idx-1] += 1  \n",
    "    cnt += 1\n",
    "    if cnt == 3: break\n",
    "\n",
    "print(\"Sample counts:\", sample_counts)\n",
    "print(\"Sample indices with duplicates:\", np.where(sample_counts > 1)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:44:28.617377Z",
     "start_time": "2024-04-24T13:44:28.593116Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "原始数据集的长度： 50000\n",
      "6250\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "\t[transforms.ToTensor(),\n",
    "\ttransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "\t\t\t\t\t\t\t\t\t\tdownload=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "\t\t\t\t\t\t\t\t\t\t\tsampler=BernoulliSampler(trainset, p=0.8), num_workers=2)\n",
    "\n",
    "gradloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "\t\t\t\t\t\t\t\t\t\t\tshuffle=False, num_workers=2) #to get the gradient for each epoch\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "\t\t\t\t\t\t\t\t\t\tdownload=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "\t\t\t\t\t\t\t\t\t\tshuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "\t\t\t'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "#print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "plt.show()\n",
    "\n",
    "print(\"原始数据集的长度：\", len(trainset))\n",
    "\n",
    "print(len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Net, self).__init__()\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 5)\n",
    "\t\tself.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\t\tself.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "\t\tself.fc2 = nn.Linear(120, 84)\n",
    "\t\tself.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.pool(F.relu(self.conv1(x)))\n",
    "\t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "\t\tx = x.view(-1, 16 * 5 * 5)\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = F.relu(self.fc2(x))\n",
    "\t\tx = self.fc3(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iter() returned non-iterator of type 'list'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 30\u001B[0m\n\u001B[1;32m     27\u001B[0m grad_norm_epoch[epoch] \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m1.\u001B[39m\u001B[38;5;241m/\u001B[39mn_samples) \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39msqrt(grad_norm_epoch[epoch])\n\u001B[1;32m     28\u001B[0m full_loss_epoch[epoch] \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m1.\u001B[39m\u001B[38;5;241m/\u001B[39mn_samples)\n\u001B[0;32m---> 30\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrainloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m:\n\u001B[1;32m     31\u001B[0m \t\u001B[38;5;66;03m# get the inputs & wrap them in Variable\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \tinputs, labels \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m     33\u001B[0m \tinputs, labels \u001B[38;5;241m=\u001B[39m Variable(inputs), Variable(labels)\n",
      "File \u001B[0;32m~/Development/Software/Anaconda/Anaconda/anaconda3/envs/main_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:438\u001B[0m, in \u001B[0;36mDataLoader.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator\n\u001B[1;32m    437\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 438\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Development/Software/Anaconda/Anaconda/anaconda3/envs/main_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:386\u001B[0m, in \u001B[0;36mDataLoader._get_iterator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    385\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_worker_number_rationality()\n\u001B[0;32m--> 386\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_MultiProcessingDataLoaderIter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Development/Software/Anaconda/Anaconda/anaconda3/envs/main_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1084\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter.__init__\u001B[0;34m(self, loader)\u001B[0m\n\u001B[1;32m   1082\u001B[0m _utils\u001B[38;5;241m.\u001B[39msignal_handling\u001B[38;5;241m.\u001B[39m_set_SIGCHLD_handler()\n\u001B[1;32m   1083\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_worker_pids_set \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m-> 1084\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfirst_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Development/Software/Anaconda/Anaconda/anaconda3/envs/main_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1117\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._reset\u001B[0;34m(self, loader, first_iter)\u001B[0m\n\u001B[1;32m   1115\u001B[0m \u001B[38;5;66;03m# prime the prefetch loop\u001B[39;00m\n\u001B[1;32m   1116\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prefetch_factor \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_workers):\n\u001B[0;32m-> 1117\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_put_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Development/Software/Anaconda/Anaconda/anaconda3/envs/main_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1351\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_put_index\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1348\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prefetch_factor \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_workers\n\u001B[1;32m   1350\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1351\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1352\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m   1353\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[0;32m~/Development/Software/Anaconda/Anaconda/anaconda3/envs/main_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:620\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter._next_index\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    619\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_index\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 620\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sampler_iter\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Development/Software/Anaconda/Anaconda/anaconda3/envs/main_env/lib/python3.9/site-packages/torch/utils/data/sampler.py:283\u001B[0m, in \u001B[0;36mBatchSampler.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    281\u001B[0m batch \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size\n\u001B[1;32m    282\u001B[0m idx_in_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 283\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msampler:\n\u001B[1;32m    284\u001B[0m     batch[idx_in_batch] \u001B[38;5;241m=\u001B[39m idx\n\u001B[1;32m    285\u001B[0m     idx_in_batch \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[0;31mTypeError\u001B[0m: iter() returned non-iterator of type 'list'"
     ]
    }
   ],
   "source": [
    "n_epoch = 20\n",
    "print(len(trainloader))\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.01\n",
    "start = time.time()\n",
    "n_samples = len(trainloader)\n",
    "grad_norm_epoch = [0 for i in range(n_epoch)]\n",
    "full_loss_epoch = [0 for i in range(n_epoch)]\n",
    "\n",
    "for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
    "\t#Compute full gradient and full loss function\n",
    "\trunning_loss = 0.0\n",
    "\tnet.zero_grad()\n",
    "\tfor i, data in enumerate(gradloader, 0):\n",
    "\t\tinputs, labels = data\n",
    "\t\tinputs, labels = Variable(inputs), Variable(labels)\n",
    "\t\toutputs = net(inputs)\n",
    "\t\tloss_grad = criterion(outputs, labels)\n",
    "\t\tloss_grad.backward() # compute gradients\n",
    "\t\tfull_loss_epoch[epoch] += loss_grad.item()\n",
    "\n",
    "\t#Compute epoch grad and epoch loss\n",
    "\t#Compute grad : see the 'clip_grad_norm' PyTorch function\n",
    "\tfor para in net.parameters():\n",
    "\t\tgrad_norm_epoch[epoch] += para.grad.data.norm(2)**2 #take the (norm2)**2 of parameter tensorFloat\n",
    "\tgrad_norm_epoch[epoch] = (1./n_samples) * np.sqrt(grad_norm_epoch[epoch])\n",
    "\tfull_loss_epoch[epoch] *= (1./n_samples)\n",
    "\n",
    "\tfor i, data in enumerate(trainloader, 0):\n",
    "\t\t# get the inputs & wrap them in Variable\n",
    "\t\tinputs, labels = data\n",
    "\t\tinputs, labels = Variable(inputs), Variable(labels)\n",
    "\t\t# forward + backward + optimize\n",
    "\t\tnet.zero_grad()\n",
    "\t\toutputs = net(inputs)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\tloss.backward() # compute gradients\n",
    "\t\tfor f in net.parameters():\n",
    "\t\t\tf.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "\t\t# print statistics\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\tif i % 2500 == 2499:    # print every 2000 mini-batches\n",
    "\t\t\tprint('[%d, %5d] loss: %.3f' %\n",
    "\t\t\t\t\t(epoch + 1, i + 1, running_loss / 2500))\n",
    "\t\t\trunning_loss = 0.0\n",
    "\n",
    "end = time.time()\n",
    "print('time is : ', end - start)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "outputs = net(Variable(images))\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n",
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_abs = [i for i in range(n_epoch)]\n",
    "plt.plot(epoch_abs, full_loss_epoch)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('value of objective function ')\n",
    "plt.title('SGD : objective function evolution, log scale')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epoch_abs, grad_norm_epoch, 'ro')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('gradient norm')\n",
    "plt.title('SGD : grad norm evolution')\n",
    "plt.grid()\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "main_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
